#+TITLE: AGENDA - INTRODUCTION TO DATA SCIENCE
#+AUTHOR: Marcus Birkenkrahe
#+SUBTITLE: DSC 105 Data Structures Lyon College Fall 2025
#+STARTUP: overview hideblocks indent entitiespretty:
#+PROPERTY: header-args:R :session *R* :results output :exports both

* DONE Week 0

- [X] Course overview incl. Syllabus and infrastructure
- [X] Introduction to our tools - R, DataCamp, DataLab
- [X] First DataCamp lesson is due next week already

** Emacs + Org-mode + ESS or DataLab?

| Criterion   | Emacs + Org + ESS                  | Jupyter (DataLab)            |
|-------------+------------------------------------+------------------------------|
| *CS students* | Benefit, will follow               | Comfortable in either        |
| *Biologists*  | Likely unfamiliar; higher friction | Familiar User Interface      |
| *Undeclared*  | Risk of alienation, esp. early on  | Visual feedback/low barrier  |
| *Economists*  | Prefer notebook-style over Emacs   | Aligns with known econ tools |

* DONE Week 1: You've arrived on MaRs.
#+attr_html: :width 400px :float nil:
[[../img/mars.png]]

Today:
- [X] Review: Last week
- [X] Continue: Course Overview - DataCamp/datalab demo
- [X] *Research mixer on Tuesday, September 30 @Lyon - tinyurl.com/res-mix*
- [X] Compare: Jupyter notebook to Emacs + Org-mode + RSS
- [X] Compare: Google Colaboratory
- [ ] *Remember: DataCamp lesson deadline is Friday, 29 Aug, 1 pm*
- [ ] *First test available later today (complete by Friday 7 Sept)*

Next:
- [ ] Introduction to data science [lecture - [[https://github.com/yjwen/org-reveal/][org-reveal]]]
- [ ] Introduction to R.

** Review: Last week (0)

1. Name 1-3 tools that we're going to use in this course.
   #+begin_quote
   1) GitHub - Course materials
   2) DataCamp - Home assignment
   3) Canvas - Tests
   #+end_quote
2. How could you prepare a little before coming to class?
   #+begin_quote
   You could prepare by:
   - Looking through your own (digital or analog) notes
   - Looking at the whiteboard screenshots (available in GDrive)
   - Looking at the last lecture file (on GitHub)
   - Look at the Zoom recording (on Canvas)
   #+end_quote
3. What do you already know about "R"?
   #+begin_quote
   - High level programming language
   - Focus on statistical functions
   - Excels at visualization and data exploration
   - The Canvas course resources contain a link to an excellent free
     online tutorial for absolute beginners (and non-programmers)
   #+end_quote
4. What can you say about GitHub?
   #+begin_quote
   - A popular platform for software developers
   - Based on Torvald's (Creator of Linux) version control program Git
   - Data science students should register, create their own portfolio
     and complete the "Hello World" program to better understand Git
   #+end_quote
5. What will you have to do to pass this class?
   #+begin_quote
   - Complete online tests (only in Canvas)
   - Complete home assignments (mostly in DataCamp)
   - Complete a project ("Sprint reviews", in GDrive)
   - Pass a short one-on-one oral exam at the end of term ("Final
     exam") - optional
   #+end_quote

** DataCamp / datalab demo

1) Open ~datacamp.com~ and log in your (Lyon) account.

2) Enter "R" in the search field at the top.

3) Click on "Introduction to R" (should appear as the top result).

4) In the course dashboard under "Resources" click on "Course Notes".

5) This will bring you to an interactive ("Jupyter") notebook with one
   command already run, =str(mtcars)=:
   #+attr_html: :width 400px :float nil:
   [[../img/datalab.png]]

6) In the next block, try the AI assistant. Enter this prompt:
   #+begin_quote
   Build a scatterplot of miles-per-gallon as a function of weight
   using mtcars.
   #+end_quote
   #+attr_html: :width 400px :float nil:
   [[../img/datalab2.png]]

7) If it looks right to you, =Accept= it - now you hcave an executable
   code chunk.
   #+attr_html: :width 400px :float nil:
   [[../img/datalab3.png]]

8) Run it with the =Play= button or with the keyboard command =CTRL +
   ENTER= - the (minimally customized) scatterplot should appear:
   #+attr_html: :width 400px :float nil:
   [[../img/datalab4.png]]

9) Create another text cell and enter:
   #+begin_quote
   Print the top of the data frame with `head()`.
   #+end_quote
   #+attr_html: :width 400px :float nil:

10) Create another code cell, enter and run:
    #+begin_example R
    head(mtcars)
    #+end_example

** Research mixer - why you should do this
#+attr_html: :width 400px :float nil:
[[../img/research_mixer.png]]

- Doing research is a great excuse to build a relationship with a
  business: People love to help, especially techies!

- This is practical networking that can help you get an internship or
  a job: You can put it on your resume, too!

- Through research, you learn to update your knowledge, and you may
  find out about interesting applications that define your career!

- Science is at crossroads for multiple reasons: It's too bloated,
  it's too expensive, it's getting too difficult, and there's AI.

** Review (Monday):
#+attr_html: :width 400px :float nil:
#+caption: Rugby scrum
[[../img/scrum.jpg]]

1. Name at least three topics that are taught on DataCamp!
   #+begin_quote
   - Languages: R, Python, SQL, Julia
   - Data dashboard/visualization apps: Tableau, Power BI, R Shiny
   - Utilities: Shell, Git, AI, LLMs
   #+end_quote
2. What is a "Jupyter notebook"?
   #+begin_quote
   A Jupyter notebook is an interactive application originally
   designed for Julia, Python, and R, that enables literate
   programming (mixing text, code, output to make programs more
   readable for humans).
   #+end_quote
3. What's a sprint review?
   #+begin_quote
   A sprint review is a meeting where the project team (you) presents
   their project prototype (whatever it is they have done so far, if
   anything), takes questions from and asks questions to the client
   (me), and explains what they're going to do in the next sprint.
   #+end_quote
4. What's the deliverable of your first sprint review?
   #+begin_quote
   The deliverable of your first sprint review on September 12, 2025,
   is a *project proposal prototype* (which means that it doesn't have
   to be perfect). You can share a prototype of the prototype with me
   before the sh** hits the fan!
   #+end_quote
5. What's IMRaD?
   #+begin_quote
   IMRaD stands for the structure of a scientific communication
   (presentation, paper, report) - *Introduction* (what did you want to
   do?), *Method* (how did you do it?), *Results* (what did you find
   out?), and *Discussion* (what do your findings mean?).

   In our application of *Scrum*, we align IMRaD with four project
   sprint reviews.
   #+end_quote
6. Can you remember one R command (from the last class)?
   #+begin_src R :session *R* :results output :exports both
     str(mtcars)  # structure of the mtcars data frame
   #+end_src

   #+RESULTS:
   #+begin_example
   'data.frame':        32 obs. of  11 variables:
    $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
    $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
    $ disp: num  160 160 108 258 360 ...
    $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
    $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
    $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
    $ qsec: num  16.5 17 18.6 19.4 17 ...
    $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
    $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
    $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
    $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
   #+end_example

   #+begin_src R :session *R* :results output :exports both
     head(mtcars)
   #+end_src

   #+RESULTS:
   :                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
   : Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
   : Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
   : Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
   : Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
   : Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
   : Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1

   #+begin_src R :file plot.png :session *R* :results graphics output file :exports both
     plot(x=mtcars$wt,y=mtcars$mpg)
   #+end_src

   #+RESULTS:
   [[file:plot.png]]

** Compare: Google Colaboratory

1) Open Google Colab at =colab.research.google.com=
2) Create a =new notebook=
3) Run =str(mtcars)= - this should give an error
4) Change =Runtime type= to R
5) Enter the command again: =str(mtcars)= and run it.
6) Plot miles-per-gallon vs. weight for all cars in =mtcars=. (You can
   use CTRL-ALT-I for a new code chunk.)
   #+begin_example R
   plot(mtcars$wt,mtcars$mpg)
   #+end_example
7) Display the top of the data frame.
   #+begin_example R
   head(mtcars)
   #+end_example
8) Interestingly, Google Gemini gets this simple task completely wrong
   (prompt"Display the top of the mtcars data frame")
   #+attr_html: :width 400px :float nil:
   [[../img/gemini.png]]
9) It manages to explain and fix the error, but its new solution is
   unnecessarily long (R does not require ~print~ here).
   #+attr_html: :width 400px :float nil:
   [[../img/gemini2.png]]

** Review (Wednesday + DataCamp)

Review questions based on class and the first DataCamp lesson. With
necessary caveats based on years of professional and life experience.

1. How does the notebook work with the R or Python console?
   #+begin_quote
   Both the notebook and the console are *front-ends* where you enter
   commands. The notebook adds structure (code cells, text cells,
   graphics). The console is just a command-line. In both cases, the
   *back-end*, the R or Python interpreter (also called REPL -
   Read-Eval-Print-Loop), does the actual work of running your code.

   - Notebook = structured, richer front-end
   - Console = plain text front-end (command-line, shell)
   - REPL = back-end that executes
   #+end_quote

2. Can you access the REPL directly when you work in a notebook?
   #+begin_quote
   Not on Google Colab or DataCamp datalab. But when you work in Emacs
   (like I do) then you can. This is very useful for debugging
   (finding and fixing errors).
   #+end_quote

3. What then is the =terminal= that I can open in DataCamp datalab?
   #+begin_quote
   The =terminal= in datalab is a shell (CLI or command-line interface)
   that allows you to talk with the operating system (in this case,
   Linux). Let's look at that.
   #+end_quote

4. What kind of relationship is =str(mtcars)= or =head(mtcars,2)=?
   #+begin_quote
   These are *function applications*: a function (like ~str~ or ~head~) is
   applied to an object (like the =mtcars= dataset). This is the same
   idea as a mathematical functional relationship f(x).
   #+end_quote

5. What is the short formula for "data science"?
   #+begin_quote
   [RAW] *DATA* + [LITERATE] *CODE* + [APPLIED] *Stats* = [DECISION] *STORY*
   #+end_quote

6. What is the core purpose of data science (according to the lesson)?
   #+begin_quote
   DataCamp: Data science takes *raw data*, *cleans it*, and *uncovers
   patterns* that help make better *predictions* and improve outcomes.

   *Caveat:* Truly "raw" data do not exist (inside a
   computer). "Cleaning" the data is always with regard to an expected
   outcome or purpose. "Patterns" is still quite abstract - can you
   think of examples for patterns? Are there any patterns around you?
   #+end_quote

7. Why is data science so important today? What did people used to do
   in the past (without "data science")?
   #+begin_quote
   DataCamp: Because we generate more data than ever before through
   online purchases, apps, social media, and smart devices, and this
   data can be used to understand the world better.

   *Caveat:* Or it could be because we're more and more out of touch
   with people and things, and have begun to trust our machines more
   than our intuition (or our elders). Perhaps it's less about
   understanding and more about selling more stuff to strangers.
   #+end_quote

8. How do streaming services like Spotify or Netflix use data science?
   #+begin_quote
   DataCamp: They analyze your watching or listening habits with
   machine learning algorithms to predict and recommend what you might
   enjoy next.

   *Caveat:* What is your personal experience with online
   recommendations? Do they generally increase your enjoyment? Are
   online devices capturing data from you without explicit approval?
   #+end_quote

9. How do recommendations systems on platforms like Amazon make
   shopping feel personalized?
   #+begin_quote
   DataCamp: They analyze browsing history, past purchases, and trends
   from similar shoppers to predict what you might need next.

   *Caveat:* What does your browsing and purchasing history really say
   about you as a person? How important is shopping in your life?
   Where is "enjoyable shopping" on the list of your top priorities?
   #+end_quote

10. What is the main focus of a data analyst? Why would a business
    need a data analyst?
    #+begin_quote
    DataCamp: Data analysts clean and organize "raw data", then create
    charts, graphs, and dashboards to explain what's happening right
    now. Businesses may need such a person because insights gained
    from data may help managers make informed, evidence-based
    decisions.

    *Caveat*: What do you think how many decisions are made in business?
    What does play a bigger role in a time-critical situation,
    data-driven analysis or gut feeling, work and life experience?
    #+end_quote

** Project proposal due in 2 weeks ("First Scrum sprint review")
#+attr_html: :width 400px :float nil:
[[../img/first_review.png]]

- Put your team mates' names, your topics and comments on the project
  page in Canvas by Friday September 12, 11:59 pm.

- Use the cancelled session on Wednesday, September 3, to meet as a
  team and work out the proposal draft.

- I have created a mock-up entry including a link to a GDrive folder
  with a fake AI-generated proposal, which is not bad at all though:
  it is specific, complete, and potentially interesting and relevant.

- *Can you use AI to generate your proposal?* Yes, but 1) you have to be
  say that and how you used AI, and 2) you need to be able to talk
  about it freely and openly, i.e. you have to make it your own.

- It is actually easier, more rewarding, more likely to succeed, and
  more instructive if you come up with your own topic and proposal.

- In the project information table, *please follow my example*:
  1) Full names of all team members!
  2) Full title of topic (or topics if you have a choice) linked
     directly to the proposal!
  3) Link to your GDrive folder (make sure it is shared across Lyon)!
     #+attr_html: :width 400px :float nil:
     [[../img/sharing.png]]

- For a similar student project from Fall 2024 that lead to a
  published paper, see:
  #+begin_quote
  Del Gobbo, C., Birkenkrahe, M. (2025). Generative AI Tools in Higher
  Education: A Case Study of Student Usage At A Small Liberal Arts College.
  In: Proc. 19th Int. Technology, Education and Development Conf (INTED25),
  3-4 March 2025, 315-324; URL: https://doi.org/10.21125/inted.2025.0142
  #+end_quote

*** Fake Proposal: Study Habits and Academic Performance
#+attr_html: :width 400px :float nil:
[[../img/fake.png]]

A project for DSC 105 (Introduction to Data Science) 
by Jim Jones, Jane Doe, and John James.

This proposal was generated by ChatGPT-5. The references are real.

**** Problem description

We want to analyze the relationship between student study habits (time
spent studying, study environment, use of groups, sleep, etc.)  and
academic performance (exam/quiz scores, GPA). The idea is to identify
which factors contribute most to success and whether there are
patterns across students.

**** Reason

This problem is interesting because study habits are something every
student can control, but it’s not always clear which strategies are
most effective. The results could provide insights for us and our
peers about how to optimize study time, sleep, and group work.

**** Constraints

- *Technical:* Collecting enough quality data may be difficult; survey
  responses may be incomplete or inaccurate.
- *Conceptual:* Correlation does not equal causation—this project can
  suggest patterns but not prove what “causes” performance.
- *Personal:* We will need to balance data collection with other
  coursework, so we may not be able to gather a large dataset.

**** Goals (and non-goals)

- *Goals:*
  1. Design and distribute a short survey, then collect at least 20
     responses.
  2. Clean and preprocess the data (handle missing values, normalize
     hours).
  3. Create summary statistics and visualizations (scatter plots,
     boxplots, bar charts).
  4. Build a simple predictive model (e.g., linear regression or
     classification).
  5. Write a short report and present findings.

- *Non-goals:*
  - We will not try to build a large-scale or professional predictive
    system.
  - We will not claim to establish causality between habits and
    grades.
  - We will not use highly complex machine learning methods (e.g.,
    neural networks).

**** Metrics

The project will be successful if:
- We collect at least 20 usable survey responses.
- Our analysis produces at least three clear, interpretable
  visualizations.
- We identify at least two meaningful relationships or patterns (e.g.,
  sleep correlates with higher GPA).
- We can present a clear and concise report with recommendations.

**** References

1. Kuh GD, Kinzie J, Buckley JA, Bridges BK, Hayek JC.
   *What Matters to Student Success: A Review of the Literature.*
   National Postsecondary Education Cooperative; 2006.

2. McGuire SY, McGuire S.
   *Teach Yourself How to Learn: Strategies You Can Use to Ace Any Course at Any Level.*
   Stylus Publishing; 2018.

3. University of California, Berkeley. "Study Strategies."
   Student Learning Center. Accessed August 2025.
   https://slc.berkeley.edu/study-strategies

**** Comments

We considered doing a project on social media usage and its effect on
grades, but decided on study habits because we can collect the data
more easily and it feels more relevant to daily student life. Our main
concern is participation—we hope enough classmates will respond
honestly to the survey.

* Week 2: Data Collection and Storage & Infrastructure (Sep 5)
#+attr_html: :width 600px :float nil:
#+caption: Data infrastructure (Source: alcorfund.com)
[[../img/infrastructure.png]]

- [ ] Projects - deadline is looming.
- [ ] Review - Data Collection and Storage & infrastructure
- [ ] Introduction to data science ("[[file:1_datascience.org::Popularity contest][Popularity Contest]]")

** Projects - 1st sprint review is coming!

- All but four of you still have to fill in the project table in
  Canvas.

- You'll get (timely) feedback from me either via chat or in the
  proposal document (if you let me edit it).

- One group project so far was going out of scope: Don't bite off more
  than you can chew. Remember: focus on data, and on what you know!

** Review (Last Friday - Aug 29 & DataCamp lesson 2)

You can find most of the answers either in the DataCamp lesson (I used
the slides to prepare these), or on the Whiteboard screenshots from
the past week.

1. What's a REPL and how does it relate to data science?
   #+begin_quote
   REPL = Read-Eval-Print-Loop - interactive programming environment
   that READs user input, EVALuates it, PRINTs the result, and LOOPs
   back for another input.
   #+end_quote
   #+attr_html: :width 600px :float nil:
   [[../img/repl.png]]
   
2. What's different when doing data science with C/C++ or with
   R/Python?
   #+begin_quote
   C/C++ are compiled languages, while R/Python are interpreted
   languages that can be used with a REPL.
   #+end_quote

3. What are web data (that are interesting to a data scientist)?
   #+begin_quote
   Web data are events (what the user did), timestamp (when he did
   it), and user information (who he is). These data are enough to
   create living process maps and identify web data flows.
   #+end_quote
   #+attr_html: :width 600px :float nil:
   [[../img/process.png]]

4. What's an API (in the context of data science)?
   #+begin_quote
   An API (Application Programming Interface) is a set of rules and
   endpoints that lets programs request and exhange data with external
   systems - e.g. Google Maps, a database of stocks, Twitter/X feeds.

   See also: Kubernetes API Server (see Google Chat).
   #+end_quote
   #+begin_src R :session *R* :results output :exports both
     ## Install if you don't already have it
     ## install.packages("httr")
     ## install.packages("jsonlite")

     library(httr)
     library(jsonlite)

     ## Example: Get a random joke from the "Official Joke API"
     url <- "https://official-joke-api.appspot.com/random_joke"

     response <- GET(url)            # Send request
     content <- content(response, "text")  # Get response as text
     data <- fromJSON(content)       # Parse JSON into an R list

     print(data$setup)               # Show joke setup
     print(data$punchline)           # Show punchline
   #+end_src

   Example output:
   #+begin_example
   : [1] "What's the difference between a seal and a sea lion?"
   : [1] "An ion! "
   #+end_example

5. What's the difference between qualitative and quantitative data?
   Examples?
   #+begin_quote
   - *Quantitative data* represent a quantity (Lat. quantus - "how
     much?"):
     + They deal with (mathematical) numbers.
     + They can be measured but not observed (directly).
     + They are highly abstracted (via math).
     + Examples: Height [in], price [$], percentage [%].

   - *Qualitative data* represent a quality (Lat. qualis - "what
     kind?"):
     + They deal with (language) descriptions
     + They can be observed but not measured (directly).
     + They are lowly abstracted (via language).
     + Examples: Tall, cheap, vague.
   #+end_quote

6. What's a relational database?
   #+begin_quote
   A relational database is software that allows you to store data in
   tables (or relations) made up of rows (or records) and columns (or
   fields) where:
   - Each table holds data about one type of thing (e.g. "students")
   - Relationships between tables are defined by keys (e.g. student ID)
   - You can define, store, control, query and combine data using SQL.
   #+end_quote

7. What's a query language? What's an example?
   #+begin_quote
   A language to articulate data queries, for example querying a table
   in a relational database management system using SQL, as in the
   query: =SELECT customer_name FROM customers;=
   #+end_quote

8. What do you need to consider when storing data?
   #+begin_quote
   - *Location* (e.g. parallel storage solutions or the cloud, or locally)
   - *Data category* (e.g. unstructured or tabular)
   - *Retrieval* (e.g. NoSQL for document (collection-based), or SQL for
     relational (table-based) databases.
   #+end_quote

9. What's the connection between structuredness of data and decision
   complexity? Compare adding prices at a cash register with deciding
   to sell your business.
   #+begin_quote
   The more complex (aka difficult, hard to describe, overwhelming) a
   decision is, the less structured are the data that support it.
   #+end_quote

10. What are the top cloud providers in the world?
    #+begin_quote
    - Amazon with Amazon Web Services (as in "Amazon Prime") ~30-40%
    - Microsoft with Azure (as in "Microsoft Windows") ~20-30%
    - Alphabet with Google Cloud (as in "Google Cloud Shell") ~10-20%
    #+end_quote

11. What does it mean to "scale data storage"?
    #+begin_quote
    When collecting large amounts of different types data from multiple
    sources, the "scaling" question is "how much, what type, and how
    fast"? If an application (e.g. for storage) scales, it can store
    small and (arbitrarily) large data volumes. To scale, technology
    either has to be invented or adapted - with computers, scale always
    comes at a price.

    E.g. for databases, you can scale vertically (bigger servers), or
    horizontally (more servers). What to do depends on your data.
    #+end_quote

12. What's a "data pipeline"?
    #+begin_quote
    A data pipeline is a workflow, a prescribed series of steps or
    stages that is used to retrieve, load, and store data, to better
    control individual steps and (ideally) automate the entire process.

    The term "pipeline" is used whenever segments are combined so that
    the output of the previous segment is the input of the next,
    e.g. in the following shell command, which pipes the string into a
    file and then counts the characters of the string:
    #+end_quote
    #+begin_src bash :results output
      echo "Hello" | tee hello.txt |  wc -c
      cat hello.txt
    #+end_src

13. What's "ETL" and how is it used (think of an example)?
    #+begin_quote
    - ETL stands for the "Extract", "Transform", "Load" data pipeline.
    - Example: autonomous vehicle operation:
      1. Retrieve real-time traffic data (e.g. "rain", "stop sign")
      2. Transform retrieved data for analysis (e.g. create table)
      3. Load transformed data into container (e.g. SQLite database)
    - When an ETL pipeline is well set up, it can be automated. In the
      case of automated vehicles, operation otherwise wouldn't be
      possible.
    #+end_quote

** Bonus: Kubernetes ([[https://chat.google.com/room/AAAALkEATEU/TnlGXFrrgt4/TnlGXFrrgt4?cls=10][see Google Chat]])
#+attr_html: :width 600px :float nil:
[[../img/kubernetes.png]]

[[https://kubernetes.io]["Kubernetes"]] (from the Greek word for "navigator" or "captain") is a
container technology - applications (like a data notebook) are
"containerized" so that the user doesn't have to worry about what's
under the hood. This is a modern version of the infrastructure setup
that I explained last week - just to show you how this looks like in
the wild:

- Kubernetes "API Server" is the front desk where your request
  goes. Now, the system will take care of your request (e.g. for data
  analysis) using any available resources (GPUs, CPUs, RAM etc.)
- The "Kubelet" is the local manager of your resources. It uses the
  "Pod Sandbox".
- Your models, data pipelines, or notebooks are packaged as Docker
  images (compressed) so that they can be reproduced. They are fetched
  from the "Image Registry".
- Each of your requests undergoes "Pod stages" (that's the job
  lifecycle)
- When you stop or the job/request is finished, the container is
  terminated, and volumes (memory) is released.

For data scientists, this setup means: Scalability (run 1 or 1000
experiments), reproducibility (environment is containerized and
identical across runs), resource efficiency (scheduled), isolation
(jobs don't compete with one another), and automation (cleanup).

* Week 3: Introduction to R (Sep 8, 10, 12)
* Week 4: Expo (Sep 15, 17, 19)
#+attr_html: :width 200px :float nil:
[[../img/expo.png]]

